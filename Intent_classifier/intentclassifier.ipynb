{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intent Classifier \n",
    "\n",
    "In this notebook a simple way to classify incoming query like \"I want a hot dog.\" into one of the intents. Finding the intent of the user query is a very important task in building a chatbot.\n",
    "\n",
    "Here intent classication is done by using a keras sequence model to extract the feature from the incoming query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2.2.4',\n",
       " '1.11.0',\n",
       " '3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)]')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras, tensorflow, sys\n",
    "keras.__version__, tensorflow.__version__, sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Dense, Input, Flatten, Lambda, Permute, GlobalMaxPooling1D, Activation, Concatenate\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Embedding, Dropout, Bidirectional, CuDNNGRU, SpatialDropout1D\n",
    "\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Dataset is taken from the link -> https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines This Dataset have been collected from different sources and have queries pertaining to 7 different intents.\n",
    "\n",
    "The dataset is given in json format and the below block of code is used to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13784, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "\n",
    "for intent in ['AddToPlaylist', 'BookRestaurant', 'GetWeather', 'PlayMusic', 'RateBook', 'SearchCreativeWork',\n",
    "               'SearchScreeningEvent']:\n",
    "\n",
    "    with open(\"./data/2017-06-custom-intent-engines/\" + intent + \"/train_\" + intent + \"_full.json\",\n",
    "              encoding='cp1251') as data_file:\n",
    "        full_data = json.load(data_file)\n",
    "        \n",
    "    texts = []\n",
    "    for i in range(len(full_data[intent])):\n",
    "        text = ''\n",
    "        for j in range(len(full_data[intent][i]['data'])):\n",
    "            text += full_data[intent][i]['data'][j]['text']\n",
    "        texts.append(text)\n",
    "\n",
    "    dftrain = pd.DataFrame(data=texts, columns=['request'])\n",
    "    dftrain[intent] = np.ones(dftrain.shape[0], dtype='int')\n",
    "\n",
    "    data = data.append(dftrain, ignore_index=True, sort=False)\n",
    "\n",
    "data = data.fillna(value=0)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample query\n",
    "\n",
    "The dataframe contains the query and the column corresponding the intent is marked 1. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>AddToPlaylist</th>\n",
       "      <th>BookRestaurant</th>\n",
       "      <th>GetWeather</th>\n",
       "      <th>PlayMusic</th>\n",
       "      <th>RateBook</th>\n",
       "      <th>SearchCreativeWork</th>\n",
       "      <th>SearchScreeningEvent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7144</th>\n",
       "      <td>Play a top five Linda Strawberry ep</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5584</th>\n",
       "      <td>What will the weather be in IN?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12905</th>\n",
       "      <td>Find the movie schedule at twelve AM.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>will the weather be colder in Naguabo four min...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989</th>\n",
       "      <td>What's the weather close to Cambodia at 05:44:13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 request  AddToPlaylist  \\\n",
       "7144                 Play a top five Linda Strawberry ep            0.0   \n",
       "5584                     What will the weather be in IN?            0.0   \n",
       "12905              Find the movie schedule at twelve AM.            0.0   \n",
       "4798   will the weather be colder in Naguabo four min...            0.0   \n",
       "3989    What's the weather close to Cambodia at 05:44:13            0.0   \n",
       "\n",
       "       BookRestaurant  GetWeather  PlayMusic  RateBook  SearchCreativeWork  \\\n",
       "7144              0.0         0.0        1.0       0.0                 0.0   \n",
       "5584              0.0         1.0        0.0       0.0                 0.0   \n",
       "12905             0.0         0.0        0.0       0.0                 0.0   \n",
       "4798              0.0         1.0        0.0       0.0                 0.0   \n",
       "3989              0.0         1.0        0.0       0.0                 0.0   \n",
       "\n",
       "       SearchScreeningEvent  \n",
       "7144                    0.0  \n",
       "5584                    0.0  \n",
       "12905                   1.0  \n",
       "4798                    0.0  \n",
       "3989                    0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Glove Embedding\n",
    "\n",
    "Load the embedding file 'glove.840B.300d.txt' and find the mean and standard deviation vectors of the word vectors. Than for all the words in the vocab initialize the corresponding word vector from the loaded embedded file. For the words for which wordvecs cannot be found in the embedding file, initialize them with a random normal distribution with the above found mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../../embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\"))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = len(word_index)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[\"request\"], data[[\"AddToPlaylist\", \"BookRestaurant\",\n",
    "                                                    \"GetWeather\", \"PlayMusic\", \"RateBook\", \"SearchCreativeWork\",\n",
    "                                                    \"SearchScreeningEvent\"]], test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and pad the text sequences\n",
    "\n",
    "Tokenize -> change the word to there integer ids\n",
    "\n",
    "Pad -> Trim or pad with zeros to make all sentences of same length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = list(X_train)\n",
    "\n",
    "# tokenize input strings\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "# prune each sentence to maximum of 100 words.\n",
    "max_sent_len = 100\n",
    "\n",
    "# sentences with less than 100 words, will be padded with zeroes to make it of length 100\n",
    "# sentences with more than 100 words, will be pruned to 100.\n",
    "X_train = pad_sequences(X_train, maxlen=max_sent_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_sent_len)\n",
    "\n",
    "embedding_matrix = load_glove(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converte the one hot vectors of class labels into numerical labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.argmax(np.array(y_train), axis=-1)\n",
    "y_test = np.argmax(np.array(y_test), axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Using a preloaded glove vectors as embedding weights for the model.\n",
    "\n",
    "Embedded word vectors are first featurized with 1D convolution and than passed to bidirectional GRU. GRU takes care of the sequential inforamtion, while CNN improved the embeddings by emphasizing on neighbor inforamtion. \n",
    "\n",
    "Global max pool layer pools 1 feature from each of the feature vector, unlike maxpool where we determine how many values is to be pooled.\n",
    "\n",
    "Features are enriched with concatenating Self-attented features of the RNN output. \n",
    "\n",
    "Finally multiple fully-connected layers are used to classify the incoming query into one of the possible intents.\n",
    "\n",
    "Adam optimizer and sparse categorical crossentropy loss are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(filters=256, activation=\"tanh\", padding=\"same\", strides=1, kernel_size=3)`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 300)     2929200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 100, 256)     230656      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100, 256)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 128)     123648      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 100, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100, 1)       129         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 1, 100)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attn_softmax (Activation)       (None, 1, 100)       0           permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1, 128)       0           attn_softmax[0][0]               \n",
      "                                                                 activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          32896       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 32)           4128        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 7)            231         dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,320,888\n",
      "Trainable params: 3,320,888\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "\n",
    "sequence_input = Input(shape=(max_sent_len,), dtype='int32')\n",
    "\n",
    "words = Embedding(input_dim=embedding_matrix.shape[0], output_dim=embedding_matrix.shape[1], weights=[embedding_matrix],\n",
    "                  trainable=True)(sequence_input)\n",
    "words = Dropout(rate=0.3)(words)\n",
    "\n",
    "output = Convolution1D(filters=256, filter_length=3, activation=\"tanh\", padding='same', strides=1)(words)\n",
    "output = Dropout(rate=0.3)(output)\n",
    "\n",
    "output = Bidirectional(CuDNNGRU(units=64, return_sequences=True), merge_mode='concat')(output)\n",
    "output_h = Activation('tanh')(output)\n",
    "\n",
    "output1 = GlobalMaxPooling1D()(output_h) \n",
    "\n",
    "# Applying attention to RNN output\n",
    "output = Dense(units=1)(output_h)\n",
    "output = Permute((2, 1))(output)\n",
    "output = Activation('softmax', name=\"attn_softmax\")(output)\n",
    "output = Lambda(lambda x: tf.matmul(x[0], x[1])) ([output, output_h])\n",
    "output2 = Flatten() (output)\n",
    "\n",
    "# Concatenating maxpooled and self attended features.\n",
    "output = Concatenate()([output1, output2])\n",
    "output = Dropout(rate=0.3)(output)\n",
    "\n",
    "output = Dense(units=128, activation='tanh')(output)\n",
    "output = Dropout(rate=0.3)(output)\n",
    "\n",
    "output = Dense(units=32, activation='tanh')(output)\n",
    "output = Dense(units=7, activation='softmax')(output)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=output)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10338/10338 [==============================] - 6s 628us/step - loss: 0.7464 - acc: 0.7519\n",
      "Epoch 2/10\n",
      "10338/10338 [==============================] - 4s 378us/step - loss: 0.1104 - acc: 0.9742\n",
      "Epoch 3/10\n",
      "10338/10338 [==============================] - 4s 383us/step - loss: 0.0646 - acc: 0.9847\n",
      "Epoch 4/10\n",
      "10338/10338 [==============================] - 4s 381us/step - loss: 0.0472 - acc: 0.9891\n",
      "Epoch 5/10\n",
      "10338/10338 [==============================] - 4s 380us/step - loss: 0.0389 - acc: 0.9900\n",
      "Epoch 6/10\n",
      "10338/10338 [==============================] - 4s 385us/step - loss: 0.0288 - acc: 0.9928\n",
      "Epoch 7/10\n",
      "10338/10338 [==============================] - 4s 388us/step - loss: 0.0159 - acc: 0.9967\n",
      "Epoch 8/10\n",
      "10338/10338 [==============================] - 4s 384us/step - loss: 0.0173 - acc: 0.9957\n",
      "Epoch 9/10\n",
      "10338/10338 [==============================] - 4s 379us/step - loss: 0.0126 - acc: 0.9968\n",
      "Epoch 10/10\n",
      "10338/10338 [==============================] - 4s 379us/step - loss: 0.0086 - acc: 0.9984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f5740fe978>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(X_train, np.array(y_train), epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Being able to classify intents for a query with an accuracy of 98.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score (macro): 0.9872548281477977\n",
      "accuracy_score: 0.9872315728380732\n"
     ]
    }
   ],
   "source": [
    "#get scores and predictions.\n",
    "p = model.predict(X_test)\n",
    "p = [np.argmax(i) for i in p]\n",
    "\n",
    "print(\"f1_score (macro):\", f1_score(y_test, p, average=\"macro\"))\n",
    "print(\"accuracy_score:\", accuracy_score(y_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
