{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LDA with skLearn and gensim\n",
    "\n",
    "The notebook uses skLearn and gensim packages to fetch 'n' important topics and 'm' most occuring words in each topic, grouped according to the LDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2.2.4',\n",
       " '1.11.0',\n",
       " '3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:27:44) [MSC v.1900 64 bit (AMD64)]')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras, tensorflow, sys\n",
    "keras.__version__, tensorflow.__version__, sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using easy to use 20newsgroups data from sklearn.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK stopwords for english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'why', 'few', \"hadn't\", 'theirs', 'me', 'he', 'very', \"should've\", \"wouldn't\", 'in', 'my', 'hasn', 'with', 'the', \"hasn't\", 'for', 'down', 'himself', 'we', 'doesn', \"it's\", 'm', 'have', 'that', \"aren't\", 'whom', 'where', 's', 'their', 'other', 'she', 'was', 'shouldn', 'at', 'off', 'if', \"she's\", 'don', 'what', \"you'd\", 'be', 'here', 'do', 'each', 'no', 'isn', 'is', 'from', 'ain', 'couldn', 'more', 'were', 'then', 'too', 'by', 'on', 'its', 'own', \"weren't\", 'both', 'ourselves', 'any', 'up', \"mustn't\", 'and', 'mightn', 'hadn', 'had', 'after', 'weren', 'over', 'itself', 'some', 'will', 'to', 'so', 'during', 'shan', 'under', 'yours', \"don't\", 'these', 'because', 'myself', 'you', 'which', 'until', 't', 're', 'your', 'while', \"isn't\", \"you've\", 'him', 'should', \"couldn't\", 'd', 'our', 'all', \"shouldn't\", 'once', 'of', 'further', 'before', 'ma', 'are', \"needn't\", 'am', 'an', \"didn't\", 'between', \"wasn't\", 'as', 'didn', 'it', 'most', 'did', \"mightn't\", \"that'll\", 'haven', 'about', 'o', 'only', 'won', 'there', 'needn', 'below', 'having', 'but', 'doing', 'them', 'wouldn', 'aren', \"doesn't\", 'yourself', 'again', 'her', 'just', \"haven't\", 'ours', 'his', 'll', 'this', 'being', 'nor', 'themselves', \"shan't\", 'who', 'than', 'mustn', 'now', \"won't\", 'against', 'or', 'y', 'above', 'through', 'how', 'when', \"you're\", 'herself', 'not', 'does', 'i', \"you'll\", 'yourselves', 'into', 'those', 'been', 've', 'such', 'they', 'can', 'hers', 'out', 'same', 'a', 'has', 'wasn'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using skLearn LDA for clustering into topics and finding interesting words in each topic\n",
    "\n",
    "The documents must be vectorized using countvectorizer when performing LDA clustering on data using sklearn.\n",
    "\n",
    "The count Vevorizer covertes a document by representing it as a vector of count of all the different words in the vocabulary. As one can see most of the units in a document vector will be 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "c_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_df -> if a word occurs in max_df percentage of documents, ignore those words. Ignore words that occurs in almost all the documents. eg. 'a' , 'the'. \n",
    "\n",
    "min_df -> if a word occurs in less than min_df number of dcouments, ignore those words. Ignore words that occurs in very few documents. eg. name of a person.\n",
    "\n",
    "max_features -> Consider the max_features number of words for the evaluation of topics. Words are taken by considering the ordered frequency of words across the documents (ofcourse, by ignoring the words occuring more than max_df).\n",
    "\n",
    "stop_words -> Remove english stopwords from the corpus. stop words are words like 'a', 'the', 'of', etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the above LDA with the countvectorized document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vec = c_vectorizer.fit_transform(documents)\n",
    "c_feature_names = c_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the LDA components by running the LatentDirichletAllocation function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online',\n",
    "                                learning_offset=50.,random_state=0).fit(c_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_method -> 'online' training means topics(components) will be incrementally trained on mini batches of data, rather than updating component values from the whole data at once.\n",
    "\n",
    "learning_offset -> a parameter for online training to slowly learn at the start of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Topics and top words in the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "people gun state control right guns crime states law police\n",
      "Topic 1:\n",
      "time question book years did like don space answer just\n",
      "Topic 2:\n",
      "mr line rules science stephanopoulos title current define int yes\n",
      "Topic 3:\n",
      "key chip keys clipper encryption number des algorithm use bit\n",
      "Topic 4:\n",
      "edu com cs vs w7 cx mail uk 17 send\n",
      "Topic 5:\n",
      "use does window problem way used point different case value\n",
      "Topic 6:\n",
      "windows thanks know help db does dos problem like using\n",
      "Topic 7:\n",
      "bike water effect road design media dod paper like turn\n",
      "Topic 8:\n",
      "don just like think know people good ve going say\n",
      "Topic 9:\n",
      "car new price good power used air sale offer ground\n",
      "Topic 10:\n",
      "file available program edu ftp information files use image version\n",
      "Topic 11:\n",
      "ax max b8f g9v a86 145 pl 1d9 0t 34u\n",
      "Topic 12:\n",
      "government law privacy security legal encryption court fbi technology information\n",
      "Topic 13:\n",
      "card bit memory output video color data mode monitor 16\n",
      "Topic 14:\n",
      "drive scsi disk mac hard apple drives controller software port\n",
      "Topic 15:\n",
      "god jesus people believe christian bible say does life church\n",
      "Topic 16:\n",
      "year game team games season play hockey players league player\n",
      "Topic 17:\n",
      "10 00 15 25 20 11 12 14 16 13\n",
      "Topic 18:\n",
      "armenian israel armenians war people jews turkish israeli said women\n",
      "Topic 19:\n",
      "president people new said health year university school day work\n"
     ]
    }
   ],
   "source": [
    "\n",
    "no_top_words = 10\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx))\n",
    "    print(\" \".join([c_feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA using gensim\n",
    "\n",
    "The stopwords in the document are removed for finding relevant top words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary([x.split() for x in documents])\n",
    "corpus = [dictionary.doc2bow([text for text in x.split() if text.lower() not in stopwords_en]) for x in documents] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic words are given along with there importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.070*\":\" + 0.026*\">\" + 0.005*\"-\" + 0.004*\"anonymous\" + 0.003*\"?\" + 0.003*\"RIPEM\" + 0.003*\"mail\" + 0.002*\"email\" + 0.002*\"information\" + 0.002*\"posting\"')\n",
      "(1, '0.023*\".\" + 0.008*\"|\" + 0.004*\"Gordon\" + 0.003*\"----------------------------------------------------------------------------\" + 0.003*\"surrender\" + 0.003*\"Banks\" + 0.003*\"intellect,\" + 0.003*\"shameful\" + 0.003*\"N3JXP\" + 0.003*\"\"Skepticism\"')\n",
      "(2, '0.007*\"*\" + 0.004*\"used\" + 0.003*\"use\" + 0.003*\"-\" + 0.003*\"ground\" + 0.002*\"power\" + 0.002*\"one\" + 0.002*\"using\" + 0.002*\"car\" + 0.002*\"may\"')\n",
      "(3, '0.012*\"key\" + 0.005*\"space\" + 0.004*\"launch\" + 0.003*\"keys\" + 0.003*\"algorithm\" + 0.003*\"first\" + 0.003*\"chip\" + 0.003*\"satellite\" + 0.003*\"DES\" + 0.003*\"--\"')\n",
      "(4, '0.005*\"-\" + 0.004*\"&\" + 0.004*\"Space\" + 0.004*\"University\" + 0.003*\"1993\" + 0.003*\"available\" + 0.003*\"Center\" + 0.003*\"--\" + 0.003*\"NASA\" + 0.003*\"April\"')\n",
      "(5, '0.007*\"use\" + 0.006*\"-\" + 0.006*\"get\" + 0.005*\"would\" + 0.005*\"like\" + 0.005*\"using\" + 0.005*\"know\" + 0.004*\"one\" + 0.004*\"anyone\" + 0.004*\"need\"')\n",
      "(6, '0.008*\"government\" + 0.007*\"--\" + 0.006*\"Q\" + 0.005*\"would\" + 0.004*\"President\" + 0.004*\"law\" + 0.003*\"encryption\" + 0.003*\"public\" + 0.003*\"American\" + 0.003*\"right\"')\n",
      "(7, '0.014*\"#\" + 0.003*\"|>\" + 0.003*\"people\" + 0.003*\"?\" + 0.003*\"one\" + 0.002*\"Israel\" + 0.002*\"may\" + 0.002*\"Israeli\" + 0.002*\"information\" + 0.002*\"also\"')\n",
      "(8, '0.082*\"-\" + 0.012*\"$\" + 0.005*\"!\" + 0.005*\"henrik]\" + 0.003*\"vs.\" + 0.003*\"games,\" + 0.002*\"Good\" + 0.002*\"Sharks\" + 0.002*\"Rockefeller\" + 0.002*\"Excellent\"')\n",
      "(9, '0.010*\"+\" + 0.007*\";\" + 0.006*\"-\" + 0.003*\"modem\" + 0.002*\"&\" + 0.002*\"shipping\" + 0.002*\"-1\" + 0.002*\"From:\" + 0.002*\"Apr\" + 0.002*\"]\"')\n",
      "(10, '0.009*\"->\" + 0.003*\"unit\" + 0.002*\"32-bit\" + 0.002*\"cross\" + 0.002*\"NuBus\" + 0.002*\"allocation\" + 0.002*\"linked\" + 0.002*\"GO\" + 0.002*\"NT\" + 0.002*\"Weaver\"')\n",
      "(11, '0.012*\"would\" + 0.009*\"one\" + 0.008*\"like\" + 0.008*\"think\" + 0.007*\"get\" + 0.007*\"know\" + 0.005*\"I\\'m\" + 0.005*\"--\" + 0.005*\"people\" + 0.005*\"could\"')\n",
      "(12, '0.007*\"people\" + 0.006*\"would\" + 0.004*\"may\" + 0.004*\"many\" + 0.003*\"one\" + 0.003*\"use\" + 0.002*\"make\" + 0.002*\"also\" + 0.002*\"cause\" + 0.002*\"could\"')\n",
      "(13, '0.010*\"God\" + 0.007*\"Jesus\" + 0.005*\"one\" + 0.005*\"believe\" + 0.004*\"Christian\" + 0.003*\"say\" + 0.003*\"Bible\" + 0.003*\"would\" + 0.003*\"$1\" + 0.003*\"Christians\"')\n",
      "(14, '0.007*\"Armenian\" + 0.006*\".\" + 0.005*\"Armenians\" + 0.005*\"Turkish\" + 0.005*\"people\" + 0.004*\"Jews\" + 0.004*\"said\" + 0.003*\"-\" + 0.003*\"said,\" + 0.003*\"killed\"')\n",
      "(15, '0.043*\"1\" + 0.030*\"0\" + 0.024*\"2\" + 0.014*\"3\" + 0.011*\"4\" + 0.008*\"5\" + 0.008*\"7\" + 0.006*\"6\" + 0.006*\"-\" + 0.006*\"25\"')\n",
      "(16, '0.090*\"MAX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'AX>\\'\" + 0.002*\"M\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\"`@(\" + 0.001*\"14\" + 0.001*\"Doug>\" + 0.001*\"------------\" + 0.001*\"--------\" + 0.001*\"pm)\" + 0.001*\"/lib/libX11.so\" + 0.001*\"MG9V=G9V=G9V=G9V=G9V=G9V=G9V=G9V=G9V=G9V=G9V=G9V=G9V=G9V=G9V=\" + 0.001*\"Symbol\"')\n",
      "(17, '0.030*\"*/\" + 0.027*\"/*\" + 0.024*\"=\" + 0.016*\"DB\" + 0.015*\"}\" + 0.007*\"{\" + 0.006*\"char\" + 0.005*\"int\" + 0.005*\"_/\" + 0.004*\"*\"')\n",
      "(18, '0.088*\"X\" + 0.016*\"*\" + 0.007*\"file\" + 0.005*\"window\" + 0.005*\"----------------------------------------------------------------------\" + 0.005*\"entry\" + 0.005*\"program\" + 0.005*\"available\" + 0.004*\"use\" + 0.003*\"Subject:\"')\n",
      "(19, '0.041*\"|\" + 0.009*\"/\" + 0.005*\"||\" + 0.005*\"=\" + 0.004*\"entries\" + 0.003*\"\\\\\" + 0.003*\"wire\" + 0.003*\"radar\" + 0.002*\"(\" + 0.002*\"de\"')\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "ldamodel = LdaModel(corpus, num_topics=no_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=no_top_words)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
